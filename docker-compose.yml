name: u42

# these are common to all containers
x-common: &common
  profiles:
    - gameprofile
    - grafanaprofile
    - elkprofile
  networks:
    - appnetwork
  env_file:
    - .env
  restart: always
  init: true
  tty: true
  stdin_open: true

services:
  postgres:
    <<: *common
    image: "postgres:16"
    profiles: ["gameprofile"]
    container_name: postgres
    labels:
      kompose.service.type: clusterip
    ports:
      - "5432:5432" 
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - postgres_logs:/var/log/postgresql
    environment:
      PGDATA: /var/lib/postgresql/data
    logging:
      driver: gelf
      options:
        gelf-address: "udp://${LOG_HOST}:12201"
        tag: "postgres"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U user -d transcendence -p 5432"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    <<: *common
    build: src/redis
    profiles: ["gameprofile"]
    container_name: redis
    labels:
      kompose.service.type: clusterip
    ports:
      - "6380:6379"
    volumes:
      - redis_data:/data
    logging:
      driver: gelf
      options:
        gelf-address: "udp://${LOG_HOST}:12201"
        tag: "redis"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"] 
      interval: 10s
      timeout: 5s
      retries: 5


  backend:
    <<: *common
    build: src/ft_transcendence_backend
    profiles: ["gameprofile"]
    container_name: backend
    labels:
      kompose.service.type: clusterip
      kompose.image-pull-policy: Never
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    ports:
      - "8000:8000"
    volumes:
      - django_logs:/app/logs
    command: >
      sh -c "python manage.py makemigrations &&
             python manage.py migrate &&
             daphne -b 0.0.0.0 -p 8000 backend.asgi:application"
    logging:
      driver: gelf
      options:
        gelf-address: "udp://${LOG_HOST}:12201"
        tag: "backend"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/metrics || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 10 

  game:
    <<: *common
    build: src/ft_transcendence_backend/game
    profiles: ["gameprofile"]
    container_name: game
    labels:
      kompose.service.type: clusterip
      kompose.image-pull-policy: Never
    depends_on:
      backend:
        condition: service_healthy
    command: python3 -m uvicorn pong_game:app --host 0.0.0.0 --port 8001 --reload
    ports:
      - "8001:8001"
    logging:
      driver: gelf
      options:
        gelf-address: "udp://${LOG_HOST}:12201"
        tag: "game"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8001 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  caddy:
    <<: *common
    build:
      context: ./src/
      dockerfile: caddy/Dockerfile
    profiles: ["gameprofile"]
    container_name: caddy
    labels:
      kompose.service.type: loadbalancer
      kompose.image-pull-policy: Never
    ports:
      - "8080:80"  
      - "8444:443"  
    depends_on:
      backend:
        condition: service_healthy
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - caddy_logs:/var/log/caddy
    logging:
      driver: gelf
      options:
        gelf-address: "udp://${LOG_HOST}:12201"
        tag: "caddy"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:80 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  postgres-exporter:
    <<: *common
    image: bitnami/postgres-exporter:latest
    profiles: ["grafanaprofile"]
    container_name: postgres-exporter
    labels:
      kompose.service.type: clusterip
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}?sslmode=disable"
      DISABLE_DEFAULT_METRICS: "true"
    depends_on:
      postgres:
        condition: service_healthy
      prometheus:
        condition: service_started
    ports:
    - "9187:9187"

  cloudflared:
    <<: *common
    image: cloudflare/cloudflared:latest
    profiles: ["gameprofile"]
    container_name: cloudflared
    restart: always
    labels:
      kompose.controller.type: deployment 
    logging:
      driver: gelf
      options:
        gelf-address: "udp://${LOG_HOST}:12201"
        tag: "cloudflared"
    command: tunnel --no-autoupdate run --token ${CLOUDFLARE_TUNNEL_TOKEN}
  
  grafana:
    <<: *common
    build: 
      context: ./src/grafana
      dockerfile: Dockerfile.grafana
    profiles: ["grafanaprofile"]
    container_name: grafana
    labels:
      kompose.service.type: nodeport
    ports:
      - "3000:3000"
    environment:
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_BASIC_ENABLED=true
      - ENABLE_LOGS_GRAFANA=true
      - GF_SECURITY_ADMIN_USER=${GF_SECURITY_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GF_SECURITY_ADMIN_PASSWORD}
      - GF_INSTALL_PLUGINS=redis-datasource
      - GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS=redis-datasource
    depends_on:
      redis:
        condition: service_healthy
    logging:
      driver: gelf
      options:
        gelf-address: "udp://${LOG_HOST}:12201"
        tag: "grafana"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
     
  # portainer:
  #   <<: *common
  #   build: src/portainer
  #   container_name: portainer
  #   volumes:
  #     - "/var/run/docker.sock:/var/run/docker.sock:ro"
  #     - portainer-data:/data
  #   depends_on:
  #     - caddy
  #   ports:
  #     - "9443:9443"
  #   command: ["--metrics", "--metrics-address=:9443"]
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -f -k https://localhost:9443 || exit 1"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5

  prometheus:
    <<: *common
    build: 
      context: ./src/grafana
      dockerfile: Dockerfile.prometheus
    profiles: ["grafanaprofile"]
    container_name: prometheus
    labels:
      kompose.service.type: clusterip
    ports:
      - "9090:9090"
    volumes:
      - prometheus_data:/prometheus
    logging:
      driver: gelf
      options:
        gelf-address: "udp://${LOG_HOST}:12201"
        tag: "prometheus"
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:9090/-/healthy || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  node-exporter:
    <<: *common
    image: prom/node-exporter:latest
    container_name: node-exporter
    profiles: ["grafanaprofile"]
    labels:
      kompose.service.type: clusterip
    ports:
      - "9101:9100"
    logging:
      driver: gelf
      options:
        gelf-address: "udp://${LOG_HOST}:12201"
        tag: "node-exporter"
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:9100/metrics || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
  
  alertmanager:
    <<: *common
    build:
      context: ./src/grafana
      dockerfile: Dockerfile.alertmanager
    profiles: ["grafanaprofile"]
    container_name: alertmanager
    labels:
      kompose.service.type: clusterip
    environment:
      - EMAIL_FROM=${EMAIL_FROM}
      - EMAIL_TO=${EMAIL_TO}
      - EMAIL_HOST=${EMAIL_HOST}
      - EMAIL_PORT=${EMAIL_PORT}
      - EMAIL_HOST_USER=${EMAIL_HOST_USER}
      - EMAIL_HOST_PASSWORD=${EMAIL_HOST_PASSWORD}
    ports:
      - "9093:9093"
    logging:
      driver: gelf
      options:
        gelf-address: "udp://${LOG_HOST}:12201"
        tag: "alertmanager"

  # promtail:
  #   <<: *common
  #   build:
  #     context: ./src/grafana
  #     dockerfile: Dockerfile.promtail
  #   container_name: promtail
  #   profiles: ["elkprofile"]
  #   volumes:
  #     - promtail_logs:/app/logs
  #     - postgres_logs:/var/log/postgresql
  #     - redis_logs:/var/log/redis
  #     - caddy_logs:/var/log/caddy
  #     - django_logs:/app/logs
  #     - grafana_logs:/var/log/grafana
  #   ports:
  #   - "9080:9080" 
  #   healthcheck:
  #     test: ["CMD-SHELL", "bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/9080; exit $?;'"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5

  kibana:
    <<: *common
    image: kibana:8.17.0
    container_name: kibana
    profiles: ["elkprofile"]
    restart: always
    labels:
      kompose.service.type: nodeport
    ports:
    - '5601:5601'
    environment:
      - ELASTICSEARCH_URL=http://elasticsearch:9200  
    depends_on:
      elasticsearch:
        condition: service_healthy
    volumes:
      - kibana_data:/usr/share/kibana/data 
    healthcheck:
      test: ["CMD-SHELL", "curl -s -f http://localhost:5601/api/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  
  elasticsearch:
    <<: *common
    image: elasticsearch:8.17.0
    profiles: ["elkprofile"]
    container_name: elasticsearch
    restart: always
    labels:
      kompose.service.type: clusterip
    volumes:
    - elastic_data:/usr/share/elasticsearch/data/
    environment:
      - ES_JAVA_OPTS=-Xmx256m -Xms256m
      - xpack.monitoring.collection.enabled=true
      - discovery.type=single-node
      - xpack.security.enabled=false 
    ports:
    - '9200:9200'
    - '9300:9300'
    healthcheck:
      test: ["CMD-SHELL", "curl -s -f http://localhost:9200/_cluster/health?wait_for_status=yellow || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    
  logstash:
    <<: *common
    container_name: logstash
    build: ./src/elk/logstash
    profiles: ["elkprofile"]
    restart: always
    labels:
      kompose.service.type: clusterip
    command: logstash -f /logstash_dir/logstash.conf 
    ports:
      - "5000:5000/udp" 
      - "9600:9600"
      - "12201:12201/udp"
    environment:
      - ES_JAVA_OPTS=-Xmx1g -Xms1g
      - xpack.monitoring.enabled=true
      - xpack.monitoring.elasticsearch.hosts=http://elasticsearch:9200

volumes:
  caddy_logs: null
  postgres_data: null
  postgres_logs: null
  redis_data: null
  redis_logs: null
  django_logs: null
  grafana_data: null
  grafana_logs: null
  grafana_provisioning: null
  prometheus_data: null
  portainer-data: null
  elastic_data: null
  kibana_data: null
  # logstash-persistence: null
  # elasticsearch_data: null
  # promtail_logs: null


networks:
  appnetwork:
    name: appnetwork
    driver: bridge